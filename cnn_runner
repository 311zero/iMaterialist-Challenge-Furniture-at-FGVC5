import argparse
import os
os.environ['CUDA_VISIBLE_ORDER'] = 'PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES'] = '1'

from tqdm import tqdm
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.autograd import Variable

import imaterialist.models as models
import imaterialist.utils as utils
from imaterialist.utils import RunningMean, use_gpu
from imaterialist.misc import FurnitureDataset, preprocess, preprocess_with_augmentation, NB_CLASSES, preprocess_hflip


#
BATCH_SIZE = 16


def get_model():
    # end：附加在最后一个值之后的字符串，默认为换行符。
    # flush:在简单页面中，该属性不纳入考虑，而在页面包含大量数据时，为缩短客户端延迟，可将一部分内容先行输出
    print('[+] loading model... ', end='', flush=True)
    model = models.densenet201_finetune(NB_CLASSES)
    if use_gpu:
        model.cuda()
    print('done')
    return model


def predict():
    model = get_model()
    # 加载模型参数
    model.load_state_dict(torch.load('best_val_weight.pth'))
    model.eval()  # 框架会自动把BN和DropOut固定住，不会取平均，而是用训练好的值

    tta_preprocess = [preprocess, preprocess_hflip]

    data_loaders = []
    for transform in tta_preprocess:
        test_dataset = FurnitureDataset('test', transform=transform)
        data_loader = DataLoader(dataset=test_dataset, num_workers=1,
                                 batch_size=BATCH_SIZE,
                                 shuffle=False)
        data_loaders.append(data_loader)

    lx, px = utils.predict_tta(model, data_loaders)
    data = {
        'lx': lx.cpu(),
        'px': px.cpu(),
    }
    torch.save(data, 'test_prediction.pth')

    data_loaders = []
    for transform in tta_preprocess:
        test_dataset = FurnitureDataset('val', transform=transform)
        data_loader = DataLoader(dataset=test_dataset, num_workers=1,
                                 batch_size=BATCH_SIZE,
                                 shuffle=False)
        data_loaders.append(data_loader)

    lx, px = utils.predict_tta(model, data_loaders)
    data = {
        'lx': lx.cpu(),  # 将数据迁移到cpu上面
        'px': px.cpu(),
    }
    torch.save(data, 'val_prediction.pth')


def train():
    train_dataset = FurnitureDataset('train', transform=preprocess_with_augmentation)
    val_dataset = FurnitureDataset('val', transform=preprocess)
    # PyTorch用类torch.utils.data.DataLoader加载数据，并对数据进行采样，生成batch迭代器
    # dataset：Dataset类型，从其中加载数据
    # shuffle：bool，可选。为True时表示每个epoch都对数据进行洗牌
    # sampler：Sampler，可选。从数据集中采样样本的方法。
    # num_workers：int，可选。加载数据时使用多少子进程。默认值为0，表示在主进程中加载数据。
    # drop_last：bool，可选。True表示如果最后剩下不完全的batch,丢弃。默认为False表示不丢弃。
    training_data_loader = DataLoader(dataset=train_dataset, num_workers=8,
                                      batch_size=BATCH_SIZE,
                                      shuffle=True)
    validation_data_loader = DataLoader(dataset=val_dataset, num_workers=1,
                                        batch_size=BATCH_SIZE,
                                        shuffle=False)

    model = get_model()
    # 定义求误差方法.  为了解决参数更新效率下降这一问题，我们使用交叉熵代价函数替换传统的平方误差函数
    # 使用默认的，输出是标量而且是平均值
    # 损失函数运行在gpu上面
    criterion = nn.CrossEntropyLoss().cuda()
    # 总共要学习的参数个数
    nb_learnable_params = sum(p.numel() for p in model.fresh_params())
    print(f'[+] nb learnable params {nb_learnable_params}')
    # 初始化loss为无穷大量
    min_loss = float("inf")
    lr = 0
    patience = 0
    for epoch in range(26):
        print(f'epoch {epoch}')
        if epoch == 1:
            lr = 0.00003
            print(f'[+] set lr={lr}')
        if patience == 2:
            patience = 0
            # 加载模型参数
            model.load_state_dict(torch.load('best_val_weight.pth'))
            lr = lr / 10
            print(f'[+] set lr={lr}')
        if epoch == 0:
            lr = 0.002
            print(f'[+] set lr={lr}')
            # 定义优化方法,方法=Adam,网络参数=model.fresh_params(),学习率=lr
            optimizer = torch.optim.Adam(model.fresh_params(), lr=lr)
        else:
            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0002)

        running_loss = RunningMean()  # 实例化类,两个对象
        running_score = RunningMean()

        model.train()
        # tqdm可以在长循环中添加一个进度提示信息
        pbar = tqdm(training_data_loader, total=len(training_data_loader))
        for inputs, labels in pbar:
            batch_size = inputs.size(0)

            inputs = Variable(inputs)
            labels = Variable(labels)
            if use_gpu:
                # 将数据放在gpu上面
                inputs = inputs.cuda()
                labels = labels.cuda()
            # 梯度初始化
            optimizer.zero_grad()
            outputs = model(inputs)  # (1)forward
            # 返回每一行中最大值的那个元素，且返回索引,preds就是索引
            _, preds = torch.max(outputs.data, dim=1)

            loss = criterion(outputs, labels)  # (2)计算误差
            running_loss.update(float(loss.data[0]), 1)
            running_score.update(torch.sum(preds != labels.data), batch_size)

            loss.backward()  # (3)backward
            optimizer.step()  # (4)更新

            pbar.set_description(f'running_loss={running_loss.value:.5f} {running_score.value:.3f}')
        print(f'[+] epoch {epoch} {running_loss.value:.5f} {running_score.value:.3f}')

        lx, px = utils.predict(model, validation_data_loader)
        log_loss = criterion(Variable(px), Variable(lx))
        log_loss = float(log_loss.data[0])

        _, preds = torch.max(px, dim=1)
        accuracy = torch.mean((preds != lx).float())
        print(f'[+] val log_loss={log_loss:.5f} accuracy={accuracy:.3f}')

        if log_loss < min_loss:
            # 仅保存模型参数(推荐使用)
            torch.save(model.state_dict(), 'best_val_weight.pth')
            print(f'[+] val score improved from {min_loss:.5f} to {log_loss:.5f}. Saved!')
            min_loss = log_loss
            patience = 0
        else:
            patience += 1


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('mode', choices=['train', 'predict'])  # 输入参数选择
    args = parser.parse_args()
    print(f'[+] start `{args.mode}`')
    if args.mode == 'train':
        train()
    elif args.mode == 'predict':
        predict()
